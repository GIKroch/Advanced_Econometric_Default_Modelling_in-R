---
title: "Probit in Corporate Bankruptcy Prediction - Modified Ohlson's Model"
author: "Grzegorz Krochmal and Katarzyna Krynska"
output:
  pdf_document: default
header-includes:
    - \usepackage{setspace}\onehalfspacing
    - \usepackage{float}
    - \usepackage{indentfirst}
fontsize: 12pt
indent: true 
abstract: "This paper tries to adjust popular corporate bankruptcy model - Ohlson's Model which was created in 1980 to the modern applications. We use Polish companies bankruptcy Data Set which was created by researchers from Wroclaw University of Science and Technology. In our analysis we switched from logit which was used in original model to probit. The results of the modelling occured to be satisfying. Model passed important tests and the obtained coefficients' and marginal effects' signs were consistent with the theory and intuition. Although the Modified Ohlson's Model performance is good there is a place for its improvement with adding some additional variables selected with modern computational tools (e.g. stepwise feature selection). " 

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
library(kableExtra) # Nice tables
library(mice) # Multivariate Imputation
library(reshape) # Melt
library(ggplot2) # Charts
library(stargazer) # very nice tables
library(sandwich) # vcovHC
library(lmtest) #coeftest
library(mfx) #probitmfx
library(DescTools) #PseudoR2
library(ResourceSelection) #gof
library(caret)

options(knitr.table.format = "latex")
```


# 1. Introduction  

In the business world it is important to know if the company you are dealing with is now and is going to be in the future in a good financial condition. Therefore, the way to predict the company's default has been the subject of interest for researchers for many years. The first important model to predict corporate bancruptcy was Altman's Z-Score model from 1968 (Altman, 1968). Since than many papers occured which were focusing this issue with different approaches. The techniques has evolved from Multidimensional Analysis proposed by Altman, through Logit (Ohlson, 1980) and Probit (Zmijewski, 1984) models, to modern solutions which incorporate highly developed Machine Learning techniques like Ensemble Boosted Trees(Tomczak et al., 2016). In our work we want to focus on Logit/Probit strategy of modelling company's default. The analysis will be based on Ohlson's model modification which we introduce to suit our data in a best possible way. Before the modelling part there is an obvious need to define our research questions:

1. *Is Ohlson's type model still a good predictor of financial default. Can it extract the whole data potential?*

2. *Do financial indicators affect the probability of company's default in line with relationships known in literature?* - We will test that by checking if all betas in our model are jointly significant. Thus our null hypothesis is as follows: H0: All betas in our model are jointly significant. After that we will see if the significant coefficients signs are as expected.

As it was stated before, it is very important for a market's health to be able to predict particular company's default with available financial indicators. It is crucial not only for investors who surely do not want to invest in companies which are going to go bankrupt but it is also really important for the whole country's economy. With the knowledge which financial indicators can predict company's default, it would allow the policy makers to concentrate on those indicators and make them  able to prevent bankruptcies. 

Some imporant works in the area of bankruptcy prediction were already mentioned. However this issue is deeply analysed in __Chapter 2__. In __Chapter 3__ there is a description of a data set we have chosen for our modelling. Also in this part we introduce data processing and feature selection to properly conduct our study. In __Chapter 4__ there is whole modelling and testing process described. In __Chapter 5__ results of analysis are included. In __Chapter 6__ there is a conclusion. 

\newpage
# 2. Literature Review

In the introduction few important works which challenged the problem of prediction of corporate bankruptcy were briefly mentioned. It all started with Altman's Multidemsional Analysis (Altman, 1968). Later numerous different ideas were introduced to solve the task of a proper prediction of bankruptcies. The works which are important for our analysis are ones which incorporate Logit (Ohlson, 1980) and Probit (Zmijewski, 1984). Obviously, it is important to state that those approaches are rather old and they do not keep up to the newest strategies. Currently, where the computing power is huge, few modern techniques of bankruptcy prediction were proposed: 

1. Rough Sets (Dimitras et al., 1999)
2. Evolutionary Programming (Zhang et al., 2013)
3. Ensemble Boosted Trees with Synthetic Features Generation (Tomczak et al., 2016)
4. Support Vector Machines (Shin et al., 2005)

However those methods can overcome the shortcomings of older approaches they are surely much more demanding. For example the SVM method, which prediction power seems to be worse only than the Ensamble Boosted Trees method, requires the function hand-tuning, which makes it a tough tool for everyday business applications (Tomczak et al., 2016).  
Coming back to the models, which are the most important for this paper, we are focusing on Ohlson's and Zmijewski's works. The Ohlson model is based on Logit. Beneath we present the list of variables Ohlson used in his analysis (Grice & Dugan, 2003):  
  
* Y = The probability of membership in the bankrupt group based on a logistic function
* X1 = log (total assets/GNP price-level index) 
* X2 = total liabilities/total assets
* X3 = working capital/total assets 
* X4 = current liabilities/current assets
* X5 = one if total liabilities exceed total assets, zero otherwise 
* X6 = net income/total assets 
* X7 = funds provided by operations/total liabilities
* X8 = one if net income was negative for the last two years, zero otherwise
* X9 = measure of change in net income,1 and Y = overall index.

\noindent It is important to indicate that Ohlson selected mentioned variables in an arbitrary way, just choosing those which were most frequently mentioned in literature. This limitation is also the case with model proposed by Zmijewski. The researcher applied such variables: 

* Y = overall index
* X1 = net income/total assets
* X2 = total debt/total assets
* X3 = current assets/current liabilities

\noindent They were all selected because of their importance in similar previous works. Additionaly, what has to be said about both models' shortcomings is the fact that the coefficients coming from the models are highly dependable on the data. So basically the models have to be retrained for every time period which is measured to obtain realiable results (Grice & Dugan, 2003). In our case it is not a significant problem but might be with huge datasets in business aplications. What might also be an important issue with those models is the fact that the dependent variables are selected arbitraty and their number is seriously limited. Nevertheless mentioned issues, both models still seem to be interesting and valuable. Due to the fact that Zmijewski's model incorporates only 3 independent variables it is Ohlson's model we focus on. 

\newpage
# 3. Data description and analysis  
  
\indent The analysed Data Set - Polish companies bankruptcy data - was created by Sebastian Tomczak and coworkers from Wroclaw University of Technology. In our analysis we used financial rates from 5th year of the forecasting period that holds information about bankruptcy status after 1 year. We chose this dataset as it is the least imbalanced of all provided. This dataset contains 64 attributes and 5910 instances (financial statements) where 410 represent bankrupted firms and 5500 companies that did not bankrupt.

```{r read_data, include=FALSE}
data <- read.arff("5year.arff")
colnames <- as.character(read.csv2("column_names.csv", header = FALSE)$V2)
colnames <- c(colnames, "class")
data$class <- as.numeric(data$class)
data$class <- ifelse(data$class==2,1,0)
```

This dataset contains missing values. Firstly, we will look at columns to see which contain the biggest number of missing values.

```{r missing_data, echo=FALSE}

na_count_cols <- data.frame(sapply(data, function(y) sum(length(which(is.na(y))))))
na_count_cols$Attr <- colnames
na_count_cols <- na_count_cols[order(na_count_cols[,1], decreasing = TRUE),]
colnames(na_count_cols) <- c("Number of missing values", "Variable description")
kable(head(na_count_cols), format = "latex", row.names = T, align = "c") %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = F ) 
data$attr37 <- NULL
```

\noindent Attribut #37 contains 2548 values, which is almost 50% of number of rows, so we will dispose this variable. 
After that, we used Multivariate Imputation by Chained Equations to impute missing data where possible. MI imputes the missing values multiple times, resulting in multiple full datasets. Then each dataset is analyzed and the results are combined.MI gives approximately unbiased estimates of all the estimates from the random error.

```{r imputation, include=FALSE, eval=FALSE}
# imputed_Data <- mice(data, m=5, maxit = 5, method = 'pmm', seed = 500)
# new_data <- complete(imputed_Data)
# write.csv2(new_data, "imputedData.csv", row.names = FALSE)
```

```{r after_imputation, echo=FALSE}
data <- read.csv2("imputedData.csv")
missing_d <- data[rowSums(is.na(data))>0,]
#sum(missing_d$class)
data <- data[rowSums(is.na(data))==0,]
```

\indent After these operations we have only 4.81% (284) of rows which contain missing data, so we will delete them. 

\newpage
# 4. Model estimation

In our analysis, we try to use models with Binary Dependent Variable, such as logit and probit. Our independent variables were chosen according to literature (Ohlson 1980). We tried to recreate all independent variables from Ohlson's research, leading us to eight independent variables:

* SIZE - Logarithm of total assets
* TLTA - Total liabilities divided by total assets
* WCTA - Working capital divided by total assets
* OENEG - One if total liabilities exceeds total assets, zero otherwise
* NITA - Net income divided by total assets
* INONE - One if net income was positive for the last year, zero otherwise
* CHSALES - Change in sales for the most recent period
* FUTL - Sum of gross profit and depreciation divided by total liabilities

```{r, echo=FALSE}
SIZE <- data$Attr29
TLTA <- data$Attr2
WCTA <- data$Attr3
OENEG <- ifelse(TLTA>=1, 1, 0)
NITA <- data$Attr1
INONE <- ifelse(NITA>0, 1, 0)
CHSALES <- data$Attr21
FUTL <- data$Attr16
BANKRUPTCY <- data$class

ohl_data <- data.frame(SIZE, TLTA, WCTA, OENEG, NITA, INONE, FUTL, CHSALES, BANKRUPTCY)

```

```{r, results='asis', echo=FALSE}
stargazer(ohl_data,  header=FALSE, type='latex', title = "Summary of analysed variables")
```
\pagebreak

\noindent After wide literature review, we expect the sign of the coefficients to be as follows:

```{r, echo=FALSE}
positive = c("TLTA", "", "", "", "", "")
negative = c("WCTA", "NITA", "INONE", "CHSALES", "SIZE", "FUTL")
indeterminate = c("OENEG", "", "", "" ,"", "")
kable(data.frame(positive, negative, indeterminate), booktabs=T, align = "c") %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = F ) 
```

## 4.1. Estimation of linear probability model (OLS with White's robust matrix), logit model and probit model

\indent The results of estimation are in Table 2. But before them there is one important issue which has to be discussed. 

```{r, echo=FALSE, results = "hide"}
lpm = lm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data)
white.lpm = vcovHC(lpm, data = olympics, type = "HC")
coeftest(lpm, vcov=sandwich)
myprobit <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="probit"))
mylogit <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="logit"))
```


\noindent After running the code for probit and logit warnings occured. We have intentionally left them visible (first two are for probit, the last one for logit). The first one is just some computional warning of the function. It is about the number of iterations the glm probit is about to handle. The other two warnings (they are actually the same warning for different functions) indicate that we might be dealing with so called perfect or quasi perfect separation. To deal with this issue we could use Logistic Regression with Firth's Bias Reduction (Firth, 1993) which should eliminate this problem. However there is a question whether we should try to deal with this at all. The perfect separation might not be the effect of some defect of our data sample but that just might be an expected outcome for a whole population. In our case we decided that perfect separation in our data can be a reflection of the whole population of companies. It is possible that the companies which go bankrupt have all the financial indicators worse than the healthy one competitors. That is why we decided to stay with standard logit/probit and omit the verion with Firth's Bias Reduction. However the Firth's approach is interesting and might be the case of interest in future studies in the area of corporate bankruptcies. 

\indent
```{r results='asis', echo=FALSE}
stargazer(lpm, myprobit, mylogit, header = F, table.placement = "H")
```

## 4.2. Choice between logit and probit on the basis of information criteria

\indent We know that our binary data is not balanced (zero values are much more frequent than one values), so we can use AIC criteria to choose between logit and probit model. The AIC criterion for probit is lower than for logistic model (look at Table 2), so in our analysis we will be using probit model.

## 4.3. Selection of significant variables; general-to-specific method to variables selection

We suspect that OENEG variable might not be signigicant. To test that, we will perform Waldtest for probit and logit model. The results of these test are in Table 3 and Table 4 respectively. P-value is above 0.05%, so we cannot reject the null hypothesis. However, removing OENEG from our model improves AIC cryteria. Therefore, we decided to remove it from the model

```{r, warning=FALSE, echo = FALSE}
myprobit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="probit"))
waldtest_probit <- waldtest(myprobit,myprobit2)
mylogit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="logit"))
waldtest_logit <- waldtest(mylogit,mylogit2)
```

```{r results='asis', echo=FALSE}
stargazer(waldtest_probit, waldtest_logit, header = F)
```

\pagebreak
## 4.4. Transformation of variables

\indent To check if any variables needs transformation, we will use loess plots for non-binary data.

```{r, out.width='33%', echo=FALSE}
cols <- c("SIZE","TLTA","WCTA","NITA","FUTL","CHSALES")
for (i in 1:length(cols)){
  plot(ohl_data[,cols[i]],predict(loess(ohl_data$BANKRUPTCY~ohl_data[,cols[i]])), title(cols[i]), xlab = "", ylab = "")
}
```

\noindent Plot for SIZE shows that an assumption of a linear effect on the logit scale, is clearly not reasonable here. We will try to include a quadratic effect of SIZE in the model.

```{r, include=FALSE}
ohl_data$SIZE2 <- ohl_data$SIZE^2
myprobit_final <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+SIZE2+CHSALES, data=ohl_data, family=binomial(link="probit"))
```

```{r, include=FALSE}
# ohl_data$FUTL_1 <- ifelse(ohl_data$FUTL<0, ohl_data$FUTL, 0)
# ohl_data$FUTL_2 <- ifelse(ohl_data$FUTL>=0, ohl_data$FUTL, 0)
# 
# myprobit_final <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL_1+FUTL_2+SIZE2+CHSALES, data=ohl_data, family=binomial(link="probit"))
```


We also suspect that there might be interactions needed in our model. We excluded the list of possible pairs that have some scientific basis:

* CHSALES:INONE
* CHSALES:NITA
* NITA:INONE
* SIZE:TLTA

Beneath we present few likelihood ratio test where Model 1 is the one with some interaction included and Model 2 is our final probit model (it is described deeper in Chapter 4.5.)
```{r interactions, warning = FALSE, echo = FALSE}
myprobit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+CHSALES:INONE, 
                 data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit2, myprobit_final)

myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+INONE+FUTL+SIZE2+CHSALES+CHSALES:NITA, 
                 data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)

myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+NITA:INONE, 
                 data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)

myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+NITA:INONE,
                 data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)
```

None of the interactions seem to be significant, so we will not include interactions in our model.

```{r interactions2, eval=FALSE, include=FALSE, warnings = F}
# BRAK INTERAKCJI
cols_to_test <- colnames(ohl_data)
cols_to_test <- cols_to_test[-9]
for (i in 1:length(cols_to_test)){
  cols_to_formula <- cols_to_test[-i]
  for (j in 1:length(cols_to_formula)){
    test_formula <- paste("BANKRUPTCY~", paste(cols_to_formula, collapse = "+"), "+", cols_to_formula[j], ":", cols_to_test[i], sep = "")
    test_probit <- suppressWarnings(glm(test_formula, data=ohl_data, family=binomial(link="probit")))
    like_test <- lrtest(test_probit, myprobit_final)
    p_val <- like_test$`Pr(>Chisq)`[2]
    if (p_val > 0.05) {
      print(paste("FOUND IT: ", paste(cols_to_formula[j], ":", cols_to_test[i], sep = "")))
    }
  }
  
}
```


## 4.5. The final model 
```{r results='asis', echo=FALSE}
stargazer(lpm, myprobit, mylogit, myprobit_final, header = F, table.placement = "H", float = T)
```

Above we present the comparison of models tested in this analysis. Where the number four is our final model.   

Now we will try to see if financial indicators affect the probability of company's default in line with relationships known in literature. We will only interpret signs of coefficiants that are statistically significant:

* TLTA - the sign of the coefficient is positive, what was expected
* WCTA - the sign of the coefficient is negative, what was expected
* NITA - the sign of the coefficient is positive, what was not expected
* INONE - the sign of the coefficient is negative, what was expected
* FUTL - the sign of the coefficient is negative, what was expected
* CHSALES - the sign of the coefficient is negative, what was expected

Not every coefficient sign is in line with the literature. A possible explanation for that is that there is some significant variable that we omitted or that there is some interaction that we missed.

## 4.6. Marginal effects for the final model

\indent We are going to calculate so called Marginal Effects at the Means
```{r, warning = F, echo = F}

meff = probitmfx(formula=BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+SIZE2+CHSALES, 
                  data=ohl_data, atmean=TRUE)
meff
```
We will consider only those marginal effects for variables which occured to be significant in regression (look Table 5). Due to the fact that the interpretation depends on the type of independent variable we will start from the only one discrete variable in our regression: 

* INONE - 1 in this variable tells us that net income for previous year was positive. So the obtained MEM tells us that if net income was positive the probability of company's default falls by 8,4 percentage points. It is intuitive result.

\noindent Other variables are continous so the interpretation will be slightly different:

* TLTA - In this case we consider total liabilites divided by total assets. If TLTA increases by 0,1 then probability of default increases by 0,1 * 0,005 which is 0,05 percentage points. It is again intuitive conclusion that companies with higher liabilities are more likely to go bankrupt. 

* WCTA - Working capital divided by total assets. In this case we find out that if WCTA increases by 0,1 then probability of default decreases by 0,1 * 0,036 which is 0,36 percentage points. Again this result is intuitive and goes with the predictions we made at the beginning of Chapter 4

* FUTL - Sum of gross profit and depreciation divided by total liabilities. If FUTL increases by 0,1 then probability of default decreases by 0,1 * 0,014 which is 0,14 percentage points. The higher gross profit the better for company's performance. This is also intuitive results

* CHSALES - Change in sales for the most recent period. If CHSALES increases by 0,1 then probability of default decreases by 0,1 * 0,0036 which is 0,036 percentage points. What we find out is that if the company achieved better performance in sales in this period than in the previous one it is less likely to go bankrupt. This is, again, an intuitive conclusion. What might be interesting and should be marked is the fact that this variable was significant in model itself but as a marginal effect got insignificant. However, due to the fact that the obtained conclusion fully complies with the theory and common sense it is still valuable for our analysis. 


## 4.7. Calculation and interpretation of odds ratios

\indent In this chapter we will analyse odds ratios of the model. We will also include 95% LR confidence intervals of odds.  
```{r, echo = F}
odds_ratios <-   exp(coef(myprobit_final))


conf_interval <- suppressWarnings(exp(confint(myprobit_final)))

res <- cbind(as.data.frame(odds_ratios), conf_interval)

res

```
\noindent Similary as in 4.6 we fill focus only on variables which were significant in the Probit model. First we will analyse the only one dependent variable which is discrete: 

* INONE - Odd ratio here tells us that the company with a positive net income for previouse year is 2 times less likely to go bankrupt than the one with a negative net income

Other variables are continous:

* TLTA - If TLTA increases by 1 the odds of company's default increases by 1,08 times

* WCTA - If WCTA increases by 1 the odds of company's default decreases by 1/0,58 = 1,72 times  

* FUTL - If FUTL increases by 1 the odds of company's default decreases by 1/0,82 = 1,22 times

* CHSALES - If CHSALES incraeses by 1 the odds of company's default decreases by 1/0,5 = 2 times



## 4.8. Linktest

\indent The next operation is to perform linktest. In this case we leave the code visible because it clarifies what the used variables are.  
```{r, warning = FALSE}
ohl_data$yhat = log(myprobit_final$fitted.values/(1-myprobit_final$fitted.values))
ohl_data$yhat2 = ohl_data$yhat^2
aux.reg = glm(BANKRUPTCY~yhat+yhat2, data=ohl_data, family=binomial(link="probit"))
summary(aux.reg)
```
\noindent It occurs that yhat2 is not significant. That indicates that the chosen model formula is correct. 

## 4.9. Intepretation of R squared

When dealing with regression where the dependent variable is not continous the interpretation of R-squared is not straightforward as in case of linear regression. To get a wider understanding of how the model we prepared fits our data we will use three different pseudo R-squared measures: Count R^2, Nagelkerke/Cragg & Uhler R^2 and McFadden's

1. Count R^2 is equal
```{r, echo = F}
# The code was taken from:
#://stats.stackexchange.com/questions/160709/how-do-you-get-count-r2-in-r-with-missing-data
countR2<-function(m) mean(m$y==round(m$fitted.values))

countR2(myprobit_final)
```
We get a really high value of Count R^2. That tells us what percent of dependent variable's value we can predict with coefficients we got from the model. However the high value is optimistic, it is unfortunatelly the limitation of Count R^2 that if one value of dependent variable occurs much more often than another one the R^2 will surely be high (Strawiński). That is why some other measures should be introduced.

\noindent As the next value we decided to use Nagelkere's Pseudo R^2 because of its [0,1] interval which makes it comfortable to interpret:
```{r, echo = F}
PseudoR2(myprobit_final,which = "Nagelkerke")
```
This value we can interpret that our model fully predicts 27% of the outcome. 

The last one measure used is McFadden's:
```{r, echo = F}
PseudoR2(myprobit_final)
```
\indent What does this value tell us about the predictive accuracy of our model? According to McFadden when his Pseudo R^2 is in range 0.2-0.4 it means a really good fit (McFadden, 1977). 

## 4.10. Hosmer-Lemeshow test

The Hosmer-Lemeshow is useful in assesing goodness of fit test for logistic regression.

```{r, echo=FALSE}
hl <- hoslem.test(myprobit_final$y, fitted(myprobit_final), g=10)
hl
```
P-value of the test is 0.2428 which is above our significance level. This means that we can not reject null hypothesis that the model is correctly specified.

## 4.11. Testing hypotheses

In the Introduction we have introduced two research questions. Therefore the hypotheses testing which comes from this questions is split for two sections. 

### 4.11.1.Hypothesis 1 testing
We asked ourselves is Ohlson's type model still a good predictor of financial default. Can it extract the whole data potential? The null hypothesis will be then:

> __H0__: *Modified Ohlson model is the best corporate bankruptcy predictor for a data we have*  

> __H1__: *The other model can be created to suit the data better* 

After getting a correct form of the model we are almost able to verify this hypothesis. However there is an obvious question how it should be conducted. We decided to create another model but which does not use any of Ohlson's assumptions but just uses the potential of the whole dataset. To create the new model we at first run a probit model with one covariate at a time to limit the number of independent variables. This way we get rid of 16 variables. The next step was dropping highly correlated variables. After that we were left with data with 34 independent variables. The final step was to run the Probit model on the modified dataset and than we used stepwise regression to find the most valuable features. The obtained model's summary is presented below. 

```{r echo = F, warning=FALSE, results = 'hide'}
data1 <- read.csv2("imputedData.csv")
missing_d <- data1[rowSums(is.na(data1))>0,]
sum(missing_d$class)
data1 <- data1[rowSums(is.na(data1))==0,]
coln <- as.character(read.csv2("column_names_1.csv", header = FALSE)$V2)
coln <- c(coln, "BANKRUPTCY")
colnames(data1) <- coln
data1$INONE <- ifelse(NITA>0, 1, 0)
data1$OENEG <- ifelse(TLTA>=1, 1, 0)
y_3 <- data1$BANKRUPTCY
data1$BANKRUPTCY <- NULL
X_3 <- data1

## To see if we can limit number of features in our model first we decided to run Logit regression with one covariate at a time. 
## If the pvalue of the variable is > 0.25 we will drop it. 

selected_names <- c()
selected_names <- lapply(names(X_3),function(name){
one_cov <- glm(y_3 ~ X_3[,name], family=binomial(link= "probit"))

if (as.numeric(summary(one_cov)$coefficients[,4][2]) < 0.25){
  selected_names <- c(selected_names, name)
}

})

selected_names <- unlist(selected_names)
data1 <- data1 %>% dplyr::select(selected_names)

## In this way we extracted 48 variables which pvalues for the regression y ~ variable[i] were less than 0,25
## It helped us to reduce the dataset by 17 variables

## The another approach to limit number of variables is to drop those which are highly correlated. 
## After the analysis of correlation data we removed next 15 variables
corr <- round(cor(X_3), 1)
to_drop <- findCorrelation(corr, cutoff = 0.9, verbose = FALSE, names = F,
                exact = ncol(corr) < 100)

data1 <- data1[,-to_drop]
data1$BANKRUPTCY <- y_3


# Fit the full model 
full.model <- glm(BANKRUPTCY ~.,data = data1, family=binomial(link= "probit"), control = list(maxit = 50))
# Stepwise regression model we do not run this in markdown cause it takes too much time. We just load this model which was previously calculated and saved
# step.model <- MASS::stepAIC(full.model, direction = "both", 
                      # trace = FALSE)
step.model <- readRDS("step_model.RDS")


```

```{r, echo = F, warning = F}
kable(broom::tidy(summary(step.model)$coefficients), row.names = FALSE, caption = "",booktabs = T) %>% kable_styling(latex_options=c("scale_down", "HOLD_position"))
```

Having a comparable model we will use Likelihood Ratio Test to see if this comparable model, with 18 variables from which 16 is significant, suits our data better than the Ohlson's type model we created before. 

```{r, echo = T}
as.data.frame(anova(myprobit_final, step.model, test= "LRT"))
```

\noindent
"step.model"" is the model obtained in this subchapter. As it occurs the pvalue of test < 0.05 so we can state that "step.model" fits our data in a better way than Ohlson's model. Therefore we reject **H0**. 

### 4.11.2. Hypothesis 2 testing
\indent
To test whether all of betas in our model are jointly significant, we will use likelihood ratio test.

```{r, echo = F}
with(myprobit_final, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```
\noindent 
P-value is close to 0, so we can reject the null hypothesis that model with predictors does not fit significantly better than a model with just an intercept. 

\newpage
# 5. Results
\indent 
Most of our modelling part was focused on building a proper modification of Ohlson's model to adjust it to our dataset but without going far from the original model. The obtained Modified Ohlson's Model performed moderately well. Out of 8 independent variables 6 were significant. Also all the calculated marginal effect and odds ratios had expected signs. The R^2 analysis also provided us with acceptable values. Additionaly the Hosmer-Lamshow test assured us that the model is correctly specified. 

Although all the tests we used on the Modified Ohlson's Model gave us rather positive view at its performance, it failed to pass our main hypothesis that it is the best model to work with such a data as we used. After conducting Loglikelihood test which compared Modified Ohlson's Model with the model based on the whole dataset, created with computational tools it occured that the Modified Ohlson's Model fits the data worse than the second model. Additionaly it is important to indicate here that the process of creating the second model was strongly simplified and it did not incorporated any theoretical analysis and was based only on computational feature selection techniques.  

\newpage
# 6. Conclusions

\indent
In our analysis, we concluded that an econometrics model can be useful in predicting whether a company will default next year. What's more, the relationships between financial indicators and the probability of default are in line with the dependencies known in litearature. 

Obviously, the shortcomings of model also occured. Ohlson created his model at the beginning of 80's so he was not able to use computational tools we can apply today. That might be the answer why Modified Ohlson's Model lost in test of fitting our data with the model created in 4.11. But it doesn't mean that Modified Ohlson's Model we created is just worse. What has to be said it is the fact that Ohlson's Model has strong theoretical basis. Also the limited number of variables make it a fast tool. This surely can't be said about the model from 4.11. which is based on stepwise regression feature selection what means that this model is really computationally demanding. Even on our rather small dataset it took this model a long time to get to final state. It means that it can be practically inapplicable when used with bigger datasets. 

To summarize the whole analysis we can say that Ohlson Model still can be valuable in the area of corporate bankruptcy prediction. However with the awareness that its coefficients should always be adjusted to modern data (Grice & Dugan, 2003). In the next studies in this area it would be useful to consider bulding partly "synthetic" Ohlson Model, which would include all the variables from the original model and a reasonable number of additional variables selected with some computational tool (e.g. stepwise regression). This approach might unleash the potential of Ohlson's Model in analysing modern problems.  


\newpage
\begin{thebibliography}{9}

\bibitem{1} 
Firth, D. (1993).
\textit{Bias Reduction of Maximum Likelihood Estimates}. 
Biometrika, Vol. 80, No. 1 (Mar., 1993), pp. 27-38. Retrieved from: https://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf

\bibitem{2}
Grice, J.S. and Dugan, M.T. (2003)
\textit{Re-Estimations of the Zmijewski and Ohlson Bankruptcy Prediction Models}.
Advances in Accounting, Volume 20, 77–93. 
Retrieved from: https://www.sciencedirect.com/science/article/pii/S0882611003200043

\bibitem{3}
McFadden, D. (1977). 
\textit{Quantitative Methods For Analyzing Travel Behaviour Of Individuals: Some Recent Developments}.
Cowles Foundation For Research In Economics At Yale University.
Retrieved from: http://cowles.yale.edu/sites/default/files/files/pub/d04/d0474.pdf

\bibitem{4}
Ohlson, J.A. (1980).
\textit{Financial Ratios and the Probabilistic Prediction of Bankruptcy}. 
Journal of Accounting Research Vol. 18 No. 1 Spring 1980. Retrieved from: https://www.jstor.org/stable/2490395

\bibitem{5}
Strawiński, P.
\textit{Analiza Wyborów Dyskretnych - Logit i logit wielomianowy}. Retrieved from: http://coin.wne.uw.edu.pl/pstrawinski/awd/awd02.pdf

\bibitem{6}
Zięba, M., Tomczak, S.K. and Tomczak, J.M. (2016).  
\textit{Ensemble Boosted Trees with Synthetic Features Generation in Application to Bankruptcy Prediction}.
Department of Computer Science, Department of Operations Research1 Faculty of Computer Science and Management, Wrocław University of Science and Technology. Retrieved from: https://www.ii.pwr.edu.pl/~tomczak/PDF/%5BMZSTJT%5D.pdf

\bibitem{7}
Zmijewski, M. E. (1984). 
\textit{Methodological Issues Related to the Estimation of Financial Distress Prediction Models}. 
Journal of Accounting Research, Vol. 22, Studies on Current Econometric Issues in Accounting Research, pp. 59-82.


\end{thebibliography}
