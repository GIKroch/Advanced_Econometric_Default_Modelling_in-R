---
title: "econometrics_model"
author: "Grzegorz Krochmal and Katarzyna Kryñska"
output:
  pdf_document: default
header-includes:
    - \usepackage{setspace}\onehalfspacing
    - \usepackage{float}
    - \usepackage{indentfirst}
    
fontsize: 12pt
indent: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
library(kableExtra) # Nice tables
library(mice) # Multivariate Imputation
library(reshape) # Melt
library(ggplot2) # Charts
library(stargazer) # very nice tables
library(sandwich) # vcovHC
library(lmtest) #coeftest
library(mfx) #probitmfx

options(knitr.table.format = "latex")
```


# 1. Introduction  

In the business world it is important to know if the company you are dealing with is now and is going to be in the future in a good financial condition. Therefore, the way to predict the company's default has been the subject of interest for researchers for many years. The first important model to predict corporate bancruptcy was Altman's Z-Score model from 1968 (Altman, 1968). Since than many papers occured which were focusing this issue with different approaches. The techniques has evolved from Multidimensional Analysis proposed by Altman, through Logit (Ohlson, 1980) and Probit (Zmijewski, 1984) models, to modern solutions which incorporate highly developed Machine Learning techniques like Ensemble Boosted Trees(Tomczak et al., 2016). In our work we want to focus on Logit/Probit strategy of modelling company's default. Before the modelling part there is an obvious need to define main and the secondary hypothesis. In our case they are:    

> \center __H0:__ *It is possible to predict company's default with specific financial indicators achieved by particular company*

> \center __H1:__ *Financial indicators can't be used as default predictors*  


As it was stated before, it is very important for a market's health to be able to predict particular company's default with available financial indicators. It is crucial not only for investors who surely do not want to invest in companies which are going to go bankrupt but it is also really important for the whole country's economy. With the knowledge what financial indicators can predict company's default it would allow the policy makers to concentrate on this indicators and so on be able to prevent bankruptcies which would be especially important in case of major companies. 

Some imporant works in the area of bankruptcy prediction were already mentioned. However this issue is deeply analysed in __Chapter 2__. In __Chapter 3__ there is a description of a data set we have chosen for our modelling. Also in this part we introduce data processing and feature selection to properly conduct our study. In __Chapter 4__ we make a choice between Probit and Logit model before running the model. 


# 2. Literature Review

In the introduction few important works which challenged the problem of prediction of corporate bankruptcy were briefly mentioned. It all started with Altman's Multidemsional Analysis (Altman, 1968). Later many different ideas were introduce to solve the task of a proper prediction of bankruptcies. The works which are important for our analysis are ones which incorporate Logit (Ohlson, 1980) and Probit (Zmijewski, 1984). Obviously, it is important to state that those approaches are rather old and they do not keep up to the newest strategies. Currently, where the computing power is huge few modern techniques of bankruptcy prediction were proposed: 

1. Rough Sets (Dimitras et al., 1999)
2. Evolutionary Programming (Zhang et al., 2013)
3. Ensemble Boosted Trees with Synthetic Features Generation (Tomczak et al., 2016)
4. Support Vector Machines (Shin et al., 2005)

However those methods can overcome the shortcomings of older approaches there are surely much more demanding. For example the SVM method which prediction power seems to be worse only than the Ensamble Boosted Trees method requires the function hand-tuning which makes it a tough tool for everyday business applications (Tomczak et al., 2016).  
Coming back to the models which are the most important for this paper we are focusing on Ohlson's And Zmijewski's works. The Ohlson model is based on Logit. Beneath we present the list of variables Ohlson used in his analysis (Grice & Dugan, 2003):  
  
* Y = The probability of membership in the bankrupt group based on a logistic function
* X1 = log (total assets/GNP price-level index) 
* X2 = total liabilities/total assets
* X3 = working capital/total assets 
* X4 = current liabilities/current assets
* X5 = one if total liabilities exceed total assets, zero otherwise 
* X6 = net income/total assets 
* X7 = funds provided by operations/total liabilities
* X8 = one if net income was negative for the last two years, zero otherwise
* X9 = measure of change in net income,1 and Y = overall index.

\setlength{\parindent}{0em} It is important to indicate that Ohlson selected mentioned variables in an arbitrary way, just choosing those which were most frequently mentioned in literature. It is some limitation which we will try to deal with in modelling part. This limitation is also the case with model proposed by Zmijewski. The researcher applied such variables: 

* Y = overall index
* X1 = net income/total assets
* X2 = total debt/total assets
* X3 = current assets/current liabilities

\setlength{\parindent}{0em} They were all selected because of their importance in similar previous works. Additionaly, what has to be said about both models' shortcomings is the fact that the coefficients coming from the models are highly dependable on the data. So basically the models have to be retrained for every time period which is measured to obtain realiable results (Grice & Dugan, 2003). In our case it is not a significant problem but might be with huge datasets in business aplications. What might also be an important issue with those models is the fact that the dependent variables are selected arbitraty and their number is seriously limited. Nevertheless mentioned issues, both models still seem to be interesting and valuable. Now it is all the matter of the dataset how well those models are going to perform and eventually how we can improve them. 

# 3. Data description and analysis

The analysed Data Set - Polish companies bankruptcy data - was created by Sebastian Tomczak and is about bankruptcy prediction of Polish companies. In our analysis we used financial rates from 5th year of the forecasting period that holds information about bankruptcy status after 1 year. We chose this dataset as it is the least imbalanced of all provided. This dataset contains 64 attributes and 5910 instances (financial statements) where 410 represent bankrupted firms and 5500 companies that did not bankrupt.

```{r read_data, include=FALSE}
data <- read.arff("5year.arff")
colnames <- as.character(read.csv2("column_names.csv", header = FALSE)$V2)
colnames <- c(colnames, "class")
data$class <- as.numeric(data$class)
data$class <- ifelse(data$class==2,1,0)
```

This dataset contains missing values. Firstly, we will look at columns to see which contain the most missing values.

```{r missing_data, echo=FALSE}

na_count_cols <- data.frame(sapply(data, function(y) sum(length(which(is.na(y))))))
na_count_cols$Attr <- colnames
na_count_cols <- na_count_cols[order(na_count_cols[,1], decreasing = TRUE),]
colnames(na_count_cols) <- c("Number of missing values", "Variable description")
kable(head(na_count_cols), format = "latex", row.names = T, align = "c") %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = F ) 
data$attr37 <- NULL
```

Attribut #37 contains 2548 values, which is almost 50% of number of rows, so we will dispose this variable. 
After that, we used Multivariate Imputation by Chained Equations to impute missing data where possible. MI imputes the missing values multiple times, resulting in multiple full datasets. Then each dataset is analyzed and the results are combined.MI gives approximately unbiased estimates of all the estimates from the random error.

```{r imputation, include=FALSE, eval=FALSE}
imputed_Data <- mice(data, m=5, maxit = 5, method = 'pmm', seed = 500)
new_data <- complete(imputed_Data)
write.csv2(new_data, "imputedData.csv", row.names = FALSE)
```

```{r after_imputation, echo=FALSE}
data <- read.csv2("imputedData.csv")
missing_d <- data[rowSums(is.na(data))>0,]
#sum(missing_d$class)
data <- data[rowSums(is.na(data))==0,]
```

After these operations we have only 4.81% (284) of rows which contain missing data, so we will delete them. 

# 4. Model estimation

In our analysis, we try to use models with Binary Dependent Variable, such as logit and probit. Our independent variables were chosen according to literature (Ohlson 1980). We tried to recreate all independent variables from Ohlson's research, leading us to eight independent variables:

* SIZE - Logarithm of total assets
* TLTA - Total liabilities divided by total assets
* WCTA - Working capital divided by total assets
* OENEG - One if total liabilities exceeds total assets, zero otherwise
* NITA - Net income divided by total assets
* INONE - One if net income was negative for the last year, zero otherwise
* CHSALES - Change in sales for the most recent period
* FUTL - Sum of gross profit and depreciation divided by total liabilities

```{r, echo=FALSE}
SIZE <- data$Attr29
TLTA <- data$Attr2
WCTA <- data$Attr3
OENEG <- ifelse(TLTA>=1, 1, 0)
NITA <- data$Attr1
INONE <- ifelse(NITA>0, 1, 0)
CHSALES <- data$Attr21
FUTL <- data$Attr16
BANKRUPTCY <- data$class

ohl_data <- data.frame(SIZE, TLTA, WCTA, OENEG, NITA, INONE, FUTL, CHSALES, BANKRUPTCY)

```

```{r, results='asis', echo=FALSE}
stargazer(ohl_data,  header=FALSE, type='latex', title = "Summary of analysed variables")
```

After wide literature review, we expect the sign of the coefficients to be as follows:

```{r, echo=FALSE}
positive = c("TLTA", "INONE", "EBIT", "", "")
negative = c("WCTA", "NITA", "CHSALES", "SIZE", "FUTL")
indeterminate = c("OENEG", "", "", "" ,"")
kable(data.frame(positive, negative, indeterminate), booktabs=T, align = "c") %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = F ) 
```

## Estimation of linear probability model (OLS with White’s robust matrix)), logit model and probit model

The results of estimation are in Table 2.

```{r, echo=FALSE}
lpm = lm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data)
white.lpm = vcovHC(lpm, data = olympics, type = "HC")
coeftest(lpm, vcov=sandwich)
myprobit <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="probit"))
mylogit <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="logit"))
```

```{r results='asis', echo=FALSE}
stargazer(lpm, myprobit, mylogit, header = F)
```

## Choice between logit and probit on the basis of information criteria

We know that our binary data are not balanced (zero values are much more frequent than one values), so we can use AIC criteria to choose between logit and probit model. The AIC criterion for probit is lower than for logistic model (look at Table 2), so in our analysis we will be using probit model.

## Selection of significant variables; general-to-specific method to variables selection

We suspect that OENEG variable might not be signigicant. To test that, we will perform Waldtest for probit and logit model. The results of these test are in Table 3 and Table 4 respectively. P-value is above 0.05%, so we cannot reject the null hypothesis. However, removing OENEG from our model improves AIC cryteria. Therefore, we decided to remove it from the model

```{r, warning=FALSE}
myprobit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="probit"))
waldtest_probit <- waldtest(myprobit,myprobit2)
mylogit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="logit"))
waldtest_logit <- waldtest(mylogit,mylogit2)
```

```{r results='asis', echo=FALSE}
stargazer(waldtest_probit, waldtest_logit, header = F)
```


## Transformation of variables

To check if any variables needs transformation, we will use loess plots for non-binary data.

```{r, out.width='33%', echo=FALSE}
cols <- c("SIZE","TLTA","WCTA","NITA","FUTL","CHSALES")
for (i in 1:length(cols)){
  plot(ohl_data[,cols[i]],predict(loess(ohl_data$BANKRUPTCY~ohl_data[,cols[i]])), title(cols[i]), xlab = "", ylab = "")
}
```

Plot for SIZE shows that an assumption of a linear effect on the logit scale, is clearly not reasonable here. We will try to include a quadratic effect of SIZE in the model.

```{r, include=FALSE}
ohl_data$SIZE2 <- ohl_data$SIZE^2
myprobit_final <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+SIZE2+CHSALES, data=ohl_data, family=binomial(link="probit"))
```

```{r, include=FALSE}
# ohl_data$FUTL_1 <- ifelse(ohl_data$FUTL<0, ohl_data$FUTL, 0)
# ohl_data$FUTL_2 <- ifelse(ohl_data$FUTL>=0, ohl_data$FUTL, 0)
# 
# myprobit_final <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL_1+FUTL_2+SIZE2+CHSALES, data=ohl_data, family=binomial(link="probit"))
```


We also suspect that there might be interactions needed in our model. We excluded the list of possible pairs that have some scientific basis:

* CHSALES:INONE
* CHSALES:NITA
* NITA:INONE
* SIZE:TLTA

```{r interactions}
myprobit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+CHSALES:INONE, data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit2, myprobit_final)
myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+INONE+FUTL+SIZE2+CHSALES+CHSALES:NITA, data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)
myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+NITA:INONE, data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)
myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+NITA:INONE, data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)
```

None of the interactions seem to be significant, so we will not include interactions in our model.

```{r interactions2, eval=FALSE, include=FALSE}
# BRAK INTERAKCJI
cols_to_test <- colnames(ohl_data)
cols_to_test <- cols_to_test[-9]
for (i in 1:length(cols_to_test)){
  cols_to_formula <- cols_to_test[-i]
  for (j in 1:length(cols_to_formula)){
    test_formula <- paste("BANKRUPTCY~", paste(cols_to_formula, collapse = "+"), "+", cols_to_formula[j], ":", cols_to_test[i], sep = "")
    test_probit <- glm(test_formula, data=ohl_data, family=binomial(link="probit"))
    like_test <- lrtest(test_probit, myprobit_final)
    p_val <- like_test$`Pr(>Chisq)`[2]
    if (p_val > 0.05) {
      print("FOUND IT: ", paste(cols_to_formula[j], ":", cols_to_test[i], sep = ""))
    }
  }
  
}
```



## The final model 

```{r results='asis', echo=FALSE}
stargazer(lpm, myprobit, mylogit, myprobit_final, header = F)
```


## Marginal effects for the final model

```{r}
# marginal effects for the average observation
(meff = probitmfx(formula=BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+SIZE2+CHSALES, data=ohl_data, atmean=TRUE))
```


## Calculation and interpretation of odds ratios



## Linktest

```{r}
ohl_data$yhat = log(myprobit_final$fitted.values/(1-myprobit_final$fitted.values))
ohl_data$yhat2 = ohl_data$yhat^2
aux.reg = glm(BANKRUPTCY~yhat+yhat2, data=ohl_data, family=binomial(link="probit"))
summary(aux.reg)
# yhat2 nieistotne, wiec dobra formula modelu
```

## Intepretation of R squared




## Hypotheses verification

## Tests
