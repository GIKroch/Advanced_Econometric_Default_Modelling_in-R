---
title: "econometrics_model"
author: "Grzegorz Krochmal and Katarzyna Kry?ska"
output:
  pdf_document: default
header-includes:
    - \usepackage{setspace}\onehalfspacing
    - \usepackage{float}
    - \usepackage{indentfirst}
fontsize: 12pt
indent: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
library(kableExtra) # Nice tables
library(mice) # Multivariate Imputation
library(reshape) # Melt
library(ggplot2) # Charts
library(stargazer) # very nice tables
library(sandwich) # vcovHC
library(lmtest) #coeftest
library(mfx) #probitmfx
library(DescTools) #PseudoR2

options(knitr.table.format = "latex")
```


# 1. Introduction  

In the business world it is important to know if the company you are dealing with is now and is going to be in the future in a good financial condition. Therefore, the way to predict the company's default has been the subject of interest for researchers for many years. The first important model to predict corporate bancruptcy was Altman's Z-Score model from 1968 (Altman, 1968). Since than many papers occured which were focusing this issue with different approaches. The techniques has evolved from Multidimensional Analysis proposed by Altman, through Logit (Ohlson, 1980) and Probit (Zmijewski, 1984) models, to modern solutions which incorporate highly developed Machine Learning techniques like Ensemble Boosted Trees(Tomczak et al., 2016). In our work we want to focus on Logit/Probit strategy of modelling company's default. Before the modelling part there is an obvious need to define main and the secondary hypothesis. In our case they are:    

> \center __H0:__ *It is possible to predict company's default with specific financial indicators achieved by particular company*

> \center __H1:__ *Financial indicators can't be used as default predictors*  


As it was stated before, it is very important for a market's health to be able to predict particular company's default with available financial indicators. It is crucial not only for investors who surely do not want to invest in companies which are going to go bankrupt but it is also really important for the whole country's economy. With the knowledge what financial indicators can predict company's default it would allow the policy makers to concentrate on this indicators and so on be able to prevent bankruptcies which would be especially important in case of major companies. 

Some imporant works in the area of bankruptcy prediction were already mentioned. However this issue is deeply analysed in __Chapter 2__. In __Chapter 3__ there is a description of a data set we have chosen for our modelling. Also in this part we introduce data processing and feature selection to properly conduct our study. In __Chapter 4__ we make a choice between Probit and Logit model before running the model. 


# 2. Literature Review

In the introduction few important works which challenged the problem of prediction of corporate bankruptcy were briefly mentioned. It all started with Altman's Multidemsional Analysis (Altman, 1968). Later many different ideas were introduce to solve the task of a proper prediction of bankruptcies. The works which are important for our analysis are ones which incorporate Logit (Ohlson, 1980) and Probit (Zmijewski, 1984). Obviously, it is important to state that those approaches are rather old and they do not keep up to the newest strategies. Currently, where the computing power is huge few modern techniques of bankruptcy prediction were proposed: 

1. Rough Sets (Dimitras et al., 1999)
2. Evolutionary Programming (Zhang et al., 2013)
3. Ensemble Boosted Trees with Synthetic Features Generation (Tomczak et al., 2016)
4. Support Vector Machines (Shin et al., 2005)

However those methods can overcome the shortcomings of older approaches there are surely much more demanding. For example the SVM method which prediction power seems to be worse only than the Ensamble Boosted Trees method requires the function hand-tuning which makes it a tough tool for everyday business applications (Tomczak et al., 2016).  
Coming back to the models which are the most important for this paper we are focusing on Ohlson's And Zmijewski's works. The Ohlson model is based on Logit. Beneath we present the list of variables Ohlson used in his analysis (Grice & Dugan, 2003):  
  
* Y = The probability of membership in the bankrupt group based on a logistic function
* X1 = log (total assets/GNP price-level index) 
* X2 = total liabilities/total assets
* X3 = working capital/total assets 
* X4 = current liabilities/current assets
* X5 = one if total liabilities exceed total assets, zero otherwise 
* X6 = net income/total assets 
* X7 = funds provided by operations/total liabilities
* X8 = one if net income was negative for the last two years, zero otherwise
* X9 = measure of change in net income,1 and Y = overall index.

\noindent It is important to indicate that Ohlson selected mentioned variables in an arbitrary way, just choosing those which were most frequently mentioned in literature. It is some limitation which we will try to deal with in modelling part. This limitation is also the case with model proposed by Zmijewski. The researcher applied such variables: 

* Y = overall index
* X1 = net income/total assets
* X2 = total debt/total assets
* X3 = current assets/current liabilities

\noindent They were all selected because of their importance in similar previous works. Additionaly, what has to be said about both models' shortcomings is the fact that the coefficients coming from the models are highly dependable on the data. So basically the models have to be retrained for every time period which is measured to obtain realiable results (Grice & Dugan, 2003). In our case it is not a significant problem but might be with huge datasets in business aplications. What might also be an important issue with those models is the fact that the dependent variables are selected arbitraty and their number is seriously limited. Nevertheless mentioned issues, both models still seem to be interesting and valuable. Now it is all the matter of the dataset how well those models are going to perform and eventually how we can improve them. 

# 3. Data description and analysis  
  
\indent The analysed Data Set - Polish companies bankruptcy data - was created by Sebastian Tomczak and is about bankruptcy prediction of Polish companies. In our analysis we used financial rates from 5th year of the forecasting period that holds information about bankruptcy status after 1 year. We chose this dataset as it is the least imbalanced of all provided. This dataset contains 64 attributes and 5910 instances (financial statements) where 410 represent bankrupted firms and 5500 companies that did not bankrupt.

```{r read_data, include=FALSE}
data <- read.arff("5year.arff")
colnames <- as.character(read.csv2("column_names.csv", header = FALSE)$V2)
colnames <- c(colnames, "class")
data$class <- as.numeric(data$class)
data$class <- ifelse(data$class==2,1,0)
```

This dataset contains missing values. Firstly, we will look at columns to see which contain the most missing values.

```{r missing_data, echo=FALSE}

na_count_cols <- data.frame(sapply(data, function(y) sum(length(which(is.na(y))))))
na_count_cols$Attr <- colnames
na_count_cols <- na_count_cols[order(na_count_cols[,1], decreasing = TRUE),]
colnames(na_count_cols) <- c("Number of missing values", "Variable description")
kable(head(na_count_cols), format = "latex", row.names = T, align = "c") %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = F ) 
data$attr37 <- NULL
```

\noindent Attribut #37 contains 2548 values, which is almost 50% of number of rows, so we will dispose this variable. 
After that, we used Multivariate Imputation by Chained Equations to impute missing data where possible. MI imputes the missing values multiple times, resulting in multiple full datasets. Then each dataset is analyzed and the results are combined.MI gives approximately unbiased estimates of all the estimates from the random error.

```{r imputation, include=FALSE, eval=FALSE}
# imputed_Data <- mice(data, m=5, maxit = 5, method = 'pmm', seed = 500)
# new_data <- complete(imputed_Data)
# write.csv2(new_data, "imputedData.csv", row.names = FALSE)
```

```{r after_imputation, echo=FALSE}
data <- read.csv2("imputedData.csv")
missing_d <- data[rowSums(is.na(data))>0,]
#sum(missing_d$class)
data <- data[rowSums(is.na(data))==0,]
```

\indent After these operations we have only 4.81% (284) of rows which contain missing data, so we will delete them. 

# 4. Model estimation

In our analysis, we try to use models with Binary Dependent Variable, such as logit and probit. Our independent variables were chosen according to literature (Ohlson 1980). We tried to recreate all independent variables from Ohlson's research, leading us to eight independent variables:

* SIZE - Logarithm of total assets
* TLTA - Total liabilities divided by total assets
* WCTA - Working capital divided by total assets
* OENEG - One if total liabilities exceeds total assets, zero otherwise
* NITA - Net income divided by total assets
* INONE - One if net income was positive for the last year, zero otherwise
* CHSALES - Change in sales for the most recent period
* FUTL - Sum of gross profit and depreciation divided by total liabilities

```{r, echo=FALSE}
SIZE <- data$Attr29
TLTA <- data$Attr2
WCTA <- data$Attr3
OENEG <- ifelse(TLTA>=1, 1, 0)
NITA <- data$Attr1
INONE <- ifelse(NITA>0, 1, 0)
CHSALES <- data$Attr21
FUTL <- data$Attr16
BANKRUPTCY <- data$class

ohl_data <- data.frame(SIZE, TLTA, WCTA, OENEG, NITA, INONE, FUTL, CHSALES, BANKRUPTCY)

```

```{r, results='asis', echo=FALSE}
stargazer(ohl_data,  header=FALSE, type='latex', title = "Summary of analysed variables")
```
\pagebreak

\noindent After wide literature review, we expect the sign of the coefficients to be as follows:

```{r, echo=FALSE}
positive = c("TLTA", "INONE", "EBIT", "", "")
negative = c("WCTA", "NITA", "CHSALES", "SIZE", "FUTL")
indeterminate = c("OENEG", "", "", "" ,"")
kable(data.frame(positive, negative, indeterminate), booktabs=T, align = "c") %>%
    kable_styling(latex_options = c("HOLD_position"), full_width = F ) 
```

## 4.1. Estimation of linear probability model (OLS with White's robust matrix), logit model and probit model

\indent The results of estimation are in Table 2. But before them there is one important issue which has to be discussed. 

```{r, echo=FALSE, results = "hide"}
lpm = lm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data)
white.lpm = vcovHC(lpm, data = olympics, type = "HC")
coeftest(lpm, vcov=sandwich)
myprobit <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="probit"))
mylogit <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="logit"))
```


\noindent After running the code for probit and logit warnings occured. We have intentionally left them visible (first two are for probit, the last one for logit). The first one is just some computional warning of the function. It is about the number of iterations the glm probit is about to handle. The other two warnings (they are actually the same warning for different functions) indicate that we might be dealing with so called perfect or quasi perfect separation. To deal with this issue we could use Logistic Regression with Firth's Bias Reduction (Firth, 1993) which which would eliminate this. However there is a question whether we should try to deal with this at all. The perfect separation might not be the effect of some problem with our data sample but that just might be an expected outcome for a whole population. In our case we decided that perfect separation in our data can be a reflection of the whole population of companies. It is possible that the companies which go bankrupt have all the financial indicators worse than the healthy one competitors. That is why we decide to stay with standard logit/probit and omit the verion with Firth's Bias Reduction. However the Firth's approach is interesting and might be the case of interest in future studies in the area of corporate bankruptcies. 

\indent
```{r results='asis', echo=FALSE}
stargazer(lpm, myprobit, mylogit, header = F)
```

## 4.2. Choice between logit and probit on the basis of information criteria

\indent We know that our binary data is not balanced (zero values are much more frequent than one values), so we can use AIC criteria to choose between logit and probit model. The AIC criterion for probit is lower than for logistic model (look at Table 2), so in our analysis we will be using probit model.

## 4.3. Selection of significant variables; general-to-specific method to variables selection

We suspect that OENEG variable might not be signigicant. To test that, we will perform Waldtest for probit and logit model. The results of these test are in Table 3 and Table 4 respectively. P-value is above 0.05%, so we cannot reject the null hypothesis. However, removing OENEG from our model improves AIC cryteria. Therefore, we decided to remove it from the model

```{r, warning=FALSE, echo = FALSE}
myprobit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="probit"))
waldtest_probit <- waldtest(myprobit,myprobit2)
mylogit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+CHSALES, data=ohl_data, family=binomial(link="logit"))
waldtest_logit <- waldtest(mylogit,mylogit2)
```

```{r results='asis', echo=FALSE}
stargazer(waldtest_probit, waldtest_logit, header = F)
```

\pagebreak
## 4.4. Transformation of variables

\indent To check if any variables needs transformation, we will use loess plots for non-binary data.

```{r, out.width='33%', echo=FALSE}
cols <- c("SIZE","TLTA","WCTA","NITA","FUTL","CHSALES")
for (i in 1:length(cols)){
  plot(ohl_data[,cols[i]],predict(loess(ohl_data$BANKRUPTCY~ohl_data[,cols[i]])), title(cols[i]), xlab = "", ylab = "")
}
```

\noindent Plot for SIZE shows that an assumption of a linear effect on the logit scale, is clearly not reasonable here. We will try to include a quadratic effect of SIZE in the model.

```{r, include=FALSE}
ohl_data$SIZE2 <- ohl_data$SIZE^2
myprobit_final <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+SIZE2+CHSALES, data=ohl_data, family=binomial(link="probit"))
```

```{r, include=FALSE}
# ohl_data$FUTL_1 <- ifelse(ohl_data$FUTL<0, ohl_data$FUTL, 0)
# ohl_data$FUTL_2 <- ifelse(ohl_data$FUTL>=0, ohl_data$FUTL, 0)
# 
# myprobit_final <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL_1+FUTL_2+SIZE2+CHSALES, data=ohl_data, family=binomial(link="probit"))
```


We also suspect that there might be interactions needed in our model. We excluded the list of possible pairs that have some scientific basis:

* CHSALES:INONE
* CHSALES:NITA
* NITA:INONE
* SIZE:TLTA

Beneath we present few likelihood ratio test where Model 1 is the one with some interaction included and Model 2 is our final probit model (it is described deeper in Chapter 4.5.)
```{r interactions, warning = FALSE, echo = FALSE}
myprobit2 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+CHSALES:INONE, 
                 data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit2, myprobit_final)

myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+INONE+FUTL+SIZE2+CHSALES+CHSALES:NITA, 
                 data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)

myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+NITA:INONE, 
                 data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)

myprobit3 <- glm(BANKRUPTCY~SIZE+TLTA+WCTA+OENEG+NITA+FUTL+SIZE2+CHSALES+NITA:INONE,
                 data=ohl_data, family=binomial(link="probit"))
lrtest(myprobit3, myprobit_final)
```

None of the interactions seem to be significant, so we will not include interactions in our model.

```{r interactions2, eval=FALSE, include=FALSE, warnings = F}
# BRAK INTERAKCJI
cols_to_test <- colnames(ohl_data)
cols_to_test <- cols_to_test[-9]
for (i in 1:length(cols_to_test)){
  cols_to_formula <- cols_to_test[-i]
  for (j in 1:length(cols_to_formula)){
    test_formula <- paste("BANKRUPTCY~", paste(cols_to_formula, collapse = "+"), "+", cols_to_formula[j], ":", cols_to_test[i], sep = "")
    test_probit <- suppressWarnings(glm(test_formula, data=ohl_data, family=binomial(link="probit")))
    like_test <- lrtest(test_probit, myprobit_final)
    p_val <- like_test$`Pr(>Chisq)`[2]
    if (p_val > 0.05) {
      print(paste("FOUND IT: ", paste(cols_to_formula[j], ":", cols_to_test[i], sep = "")))
    }
  }
  
}
```


## 4.5. The final model 
```{r results='asis', echo=FALSE}
stargazer(lpm, myprobit, mylogit, myprobit_final, header = F, table.placement = "H", float = T)
```

Above we present the comparison of models tested in this analysis. Where the number four is our final model.    

## 4.6. Marginal effects for the final model

\indent We are going to calculate so called Marginal Effects at the Means
```{r, warning = F, echo = F}

meff = probitmfx(formula=BANKRUPTCY~SIZE+TLTA+WCTA+NITA+INONE+FUTL+SIZE2+CHSALES, 
                  data=ohl_data, atmean=TRUE)
meff
```
We will consider only those marginal effects for variables which occured to be significant in regression (look Table 5). Due to the fact that the interpretation depends on the type of independent variable we will start from the only one discrete variable in our regression: 

* INONE - 1 in this variable tells us that net income for previous year was positive. So the obtained MEM tells us that if net income was positive the probability of company's default falls by 8,4 percentage points. It is intuitive result.

\noindent Other variables are continous so the interpretation will be slightly different:

* TLTA - In this case we consider total liabilites divided by total assets. If TLTA increases by 0,1 then probability of default increases by 0,1 * 0,005 which is 0,05 percentage points. It is again intuitive conclusion that companies with higher liabilities are more likely to go bankrupt. 

* WCTA - Working capital divided by total assets. In this case we find out that if WCTA increases by 0,1 then probability of default decreases by 0,1 * 0,036 which is 0,36 percentage points. Again this result is intuitive and goes with the predictions we made at the beginning of Chapter 4

* FUTL - Sum of gross profit and depreciation divided by total liabilities. If FUTL increases by 0,1 then probability of default decreases by 0,1 * 0,014 which is 0,14 percentage points. The higher gross profit the better for company's performance. This is also intuitive results

* CHSALES - Change in sales for the most recent period. If CHSALES increases by 0,1 then probability of default decreases by 0,1 * 0,0036 which is 0,036 percentage points. What we find out is that if the company achieved better performance in sales in this period than in the previous one it is less likely to go bankrupt. This is, again, an intuitive conclusion. What might be interesting and should be marked is the fact that this variable was significant in model itself but as a marginal effect got insignificant. However, due to the fact that the obtained conclusion fully complies with the theory and common sense it is still valuable for our analysis. 


## 4.7. Calculation and interpretation of odds ratios

\indent In this chapter we will analyse odds ratios of the model. We will also include 95% LR confidence intervals of odds.  
```{r, echo = F}
odds_ratios <-   exp(coef(myprobit_final))


conf_interval <- suppressWarnings(exp(confint(myprobit_final)))

res <- cbind(as.data.frame(odds_ratios), conf_interval)

res

```
\noindent Similary as in 4.6 we fill focus only on variables which were significant in the Probit model. First we will analyse the only one dependent variable which is discrete: 

* INONE - Odd ratio here tells us that the company with a positive net income for previouse year is 2 times less likely to go bankrupt than the one with a negative net income

Other variables are continous:

* TLTA - If TLTA increases by 1 the odds of company's default increases by 1,08 times

* WCTA - If WCTA increases by 1 the odds of company's default decreases by 1/0,58 = 1,72 times  

* FUTL - If FUTL increases by 1 the odds of company's default decreases by 1/0,82 = 1,22 times

* CHSALES - If CHSALES incraeses by 1 the odds of company's default decreases by 1/0,5 = 2 times



## 4.8. Linktest

\indent The next operation is to perform linktest. In this case we leave the code visible because it clarifies what the used variables are.  
```{r, warning = FALSE}
ohl_data$yhat = log(myprobit_final$fitted.values/(1-myprobit_final$fitted.values))
ohl_data$yhat2 = ohl_data$yhat^2
aux.reg = glm(BANKRUPTCY~yhat+yhat2, data=ohl_data, family=binomial(link="probit"))
summary(aux.reg)
```
\noindent It occurs that yhat2 is not significant. That indicates that the chosen model formula is correct. 

## 4.9. Intepretation of R squared

When dealing with regression where the dependent variable is not continous the interpretation of R-squared is not straightforward as in case of linear regression. To get a wider understanding of how the model we prepared fits our data we will use three different pseudo R-squared measures: Count R^2 Nagelkerke/Cragg & Uhler R^2 and McFadden's

1. Count R^2 is equal
```{r, echo = F}
# The code was taken from:
#://stats.stackexchange.com/questions/160709/how-do-you-get-count-r2-in-r-with-missing-data
countR2<-function(m) mean(m$y==round(m$fitted.values))

countR2(myprobit_final)
```
We get a really high value of Count R^2 what tells us what percent of dependent variable's value we can predict with coefficients we got from the model. However the high value is optimistic, it is unfortunatelly the limitation of Count R^2 that if one value of dependent variable occurs much more often than another one the R^2 will surely be high (StrawiÅ„ski). That is why some other measures should be introduced.

As the next value we decided to use Nagelkere's Pseudo R^2 because of its [0,1] interval which makes it comfortable to interpret:
```{r, echo = F}
PseudoR2(myprobit_final,which = "Nagelkerke")
```
This value we can interpret that our model fully predicts 27% of the outcome. 

The last one measure used is McFadden's:
```{r, echo = F}
PseudoR2(myprobit_final)
```
What does this value tell us about the predictive accuracy of our model? According to McFadden when his Pseudo R^2 is in range 0.2-0.4 it means a really good fit. And it is normal situation that this type of R^2 does not "behave" as well as this one from linear regression (McFadden, 1977)

## Hypotheses verification

## Tests

\begin{thebibliography}{9}
\bibitem{latexcompanion} 
xxx
\textit{The \LaTeX\ Companion}. 
yyy, Reading, Massachusetts, 1993.
\end{thebibliography}
